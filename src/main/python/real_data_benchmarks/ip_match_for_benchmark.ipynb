{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from io import BytesIO\n",
    "from sys import stdout\n",
    "from xml.etree.ElementTree import ElementTree, Element, SubElement, dump\n",
    "from xml.dom.minidom import parse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "\n",
    "def add_array_set(l, a, id_a):\n",
    "    \"\"\"\n",
    "    add array to list of arrays, only keep unique\n",
    "    return index of added or kept array\n",
    "    \"\"\"\n",
    "    for id_b, b in enumerate(l):\n",
    "        if np.allclose(a, b):\n",
    "            return id_b\n",
    "    l += [a]\n",
    "    return id_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD click parsing, use code below instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect list of pixel coordinates per pair\n",
    "res = {}\n",
    "\n",
    "# accumulate manual click output from multiple files\n",
    "content = ''\n",
    "files = (\n",
    "    '/Volumes/davidh-ssd/manual_ips_angle1_v1.txt',\n",
    "    '/Volumes/davidh-ssd/manual_ips_angle2.txt',\n",
    "    '/Volumes/davidh-ssd/manual_ips_mview.txt',\n",
    ")\n",
    "for file in files:\n",
    "    with open(file, 'r') as fd:\n",
    "        content += '\\n' + fd.read()\n",
    "\n",
    "# split on pair header : !vid_a-vid_b\n",
    "pair_cts = content.split('\\n!')\n",
    "p_header = re.compile('!*([0-9]+)-([0-9]+)')\n",
    "\n",
    "for pair_ct in pair_cts:\n",
    "\n",
    "    # ignore empty or commented-out content\n",
    "    if pair_ct.strip() == '' or pair_ct.strip().startswith('#'):\n",
    "        continue\n",
    "    \n",
    "    # get vid-pair\n",
    "    header = pair_ct.strip().split('\\n', 1)[0]\n",
    "    vid_a, vid_b = p_header.match(header.strip()).groups()\n",
    "    \n",
    "    res_pair = []\n",
    "    pa = re.compile('.*?tpId=0 setupId={}--- global: (\\(.*?\\))--- pixel: (\\(.*?\\)).*?'.format(vid_a))\n",
    "    pb = re.compile('.*?\\n?tpId=0 setupId={}--- global: (\\(.*?\\))--- pixel: (\\(.*?\\)).*?'.format(vid_b))\n",
    "    \n",
    "    # split on --- lines\n",
    "    lines = pair_ct.split('---\\n')[1:]\n",
    "    \n",
    "    # go over pairs of file chunks\n",
    "    for line_a, line_b in zip(*[iter(lines)]*2):\n",
    "        \n",
    "        try:\n",
    "            # parse global, pixel coords\n",
    "            gla, pxa = pa.match(line_a.strip()).groups()\n",
    "            glb, pxb = pb.match(line_b.strip()).groups()\n",
    "        except AttributeError as ex:\n",
    "            print('Error parsing {} on:\\n{}\\n{}'.format((vid_a, vid_b), line_a, line_b))\n",
    "            continue\n",
    "            \n",
    "        # add as pair of np-arrays\n",
    "        res_pair += [(np.array([*map(float, pxa.strip('()').split(','))]),\n",
    "                      np.array([*map(float, pxb.strip('()').split(','))]))]\n",
    "    \n",
    "    # add all for pair\n",
    "    res[(int(vid_a), int(vid_b))] = res_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a: Parse click log: Between images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_click_log(file, is_split=False, use_raw_split=False, split_dataset=None, fold_split=8):\n",
    "    \n",
    "    # we have a split dataset, want to use grouped views, but have not provided a dataset file\n",
    "    if is_split and not use_raw_split and split_dataset is None:\n",
    "        return {}\n",
    "        \n",
    "    # get all registrations from XML\n",
    "    if split_dataset is not None:\n",
    "        root = ElementTree().parse(split_dataset)\n",
    "        vrs = root.findall('./ViewRegistrations/ViewRegistration')\n",
    "\n",
    "    p_click = re.compile('.*?tpId=0 setupId=([0-9]+?)--- global: (\\(.*?\\))--- pixel: (\\(.*?\\)).*?')\n",
    "    p_header = re.compile('([0-9]+)-([0-9]+)')\n",
    "\n",
    "    res_pair = defaultdict(list)\n",
    "    fold_split=8\n",
    "\n",
    "    with open(file, 'r') as fd:\n",
    "        ct = fd.read()\n",
    "\n",
    "    # remove comments and empty lines\n",
    "    ct = '\\n'.join(filter(lambda l: not l.strip().startswith('#') and not l.strip()=='', ct.split('\\n')))\n",
    "    \n",
    "    # split on !headers\n",
    "    blocks = list(filter(lambda c: c.strip() != '', ct.split('!')))\n",
    "\n",
    "    \n",
    "    for block in blocks:\n",
    "\n",
    "        header, clicks = block.split('\\n', 1)\n",
    "        vid_a, vid_b = p_header.match(header.strip()).groups()\n",
    "\n",
    "        clicks = list(map(str.strip, filter(lambda c: c.strip() != '', clicks.split('---\\n'))))\n",
    "        \n",
    "        for click_a, click_b in zip(*[iter(clicks)]*2):\n",
    "\n",
    "            clia = click_a.split('\\n')\n",
    "            clib = click_b.split('\\n')\n",
    "\n",
    "            locs_a = [p_click.match(c.strip()).groups() \n",
    "                      for c in clia if (\n",
    "                          int(p_click.match(c.strip()).groups()[0]) // (fold_split if is_split else 1)\n",
    "                      ) == int(vid_a)]\n",
    "            locs_b = [p_click.match(c.strip()).groups() \n",
    "                      for c in clib if (\n",
    "                          int(p_click.match(c.strip()).groups()[0]) // (fold_split if is_split else 1)\n",
    "                      ) == int(vid_b)]\n",
    "\n",
    "            locs_a = [(int(vid), np.array([*map(float, pxa.strip('()').split(','))])) for vid, _, pxa in locs_a]\n",
    "            locs_b = [(int(vid), np.array([*map(float, pxa.strip('()').split(','))])) for vid, _, pxa in locs_b] \n",
    "\n",
    "            if not use_raw_split:\n",
    "                locs_a = locs_a[:1]\n",
    "                locs_b = locs_b[:1]\n",
    "                \n",
    "            if is_split and not use_raw_split:\n",
    "                locs_a_p = []\n",
    "                for (vi, l) in locs_a:\n",
    "                    # get view transformations of split views\n",
    "                    vt = vrs[vi].findall('./ViewTransform')[-1]\n",
    "                    affine = np.zeros((4,4))\n",
    "                    affine[:3] += np.array(list(vt)[1].text.split()).astype(np.float).reshape((3,4))\n",
    "                    affine[3,3] = 1.0\n",
    "                    lp = affine.dot(np.array(list(l) + [1]))[:3]\n",
    "                    locs_a_p.append((vi, lp))\n",
    "                locs_a = locs_a_p\n",
    "\n",
    "                locs_b_p = []\n",
    "                for (vi, l) in locs_b:\n",
    "                    vt = vrs[vi].findall('./ViewTransform')[-1]\n",
    "                    affine = np.zeros((4,4))\n",
    "                    affine[:3] += np.array(list(vt)[1].text.split()).astype(np.float).reshape((3,4))\n",
    "                    affine[3,3] = 1.0\n",
    "                    lp = affine.dot(np.array(list(l) + [1]))[:3]\n",
    "                    locs_b_p.append((vi, lp))\n",
    "                locs_b = locs_b_p\n",
    "            \n",
    "            if not use_raw_split:\n",
    "                locs_a = [(vid_a, l) for _, l in locs_a]\n",
    "                locs_b = [(vid_b, l) for _, l in locs_b] \n",
    "                # TODO: change vid to group\n",
    "\n",
    "            for ((vida, loc1), (vidb, loc2)) in product(locs_a, locs_b):\n",
    "                res_pair[(vida, vidb)].append((loc1, loc2))\n",
    "    return res_pair\n",
    "\n",
    "#file = '/Users/david/Desktop/split-manual-ips-angle1.txt'\n",
    "file = '/Volumes/davidh-ssd/manual_ips_angle1_v1.txt'\n",
    "parse_click_log(file, is_split=False, use_raw_split=False,\n",
    "                split_dataset='/Volumes/davidh-ssd/BS_TEST/dataset_prealign.split.xml')\n",
    "\n",
    "tasks = [\n",
    "    {\n",
    "        'file': '/Users/david/Desktop/split-manual-ips-angle1.txt',\n",
    "        'is_split': True,\n",
    "        'use_raw_split': False,\n",
    "        'split_dataset':'/Volumes/davidh-ssd/BS_TEST/dataset_prealign.split.xml'\n",
    "    },\n",
    "    {\n",
    "        'file': '/Users/david/Desktop/split-manual-ips-angle2.txt',\n",
    "        'is_split': True,\n",
    "        'use_raw_split': False,\n",
    "        'split_dataset':'/Volumes/davidh-ssd/BS_TEST/dataset_prealign.split.xml'\n",
    "    },\n",
    "    {\n",
    "        'file': '/Users/david/Desktop/split-manual-ips-mview.txt',\n",
    "        'is_split': True,\n",
    "        'use_raw_split': False,\n",
    "        'split_dataset':'/Volumes/davidh-ssd/BS_TEST/dataset_prealign.split.xml'\n",
    "    },\n",
    "    {\n",
    "        'file': '/Volumes/davidh-ssd/manual_ips_angle1_v1.txt',\n",
    "        'is_split': False,\n",
    "        'use_raw_split': False,\n",
    "        'split_dataset':None\n",
    "    },\n",
    "    {\n",
    "        'file': '/Volumes/davidh-ssd/manual_ips_angle2.txt',\n",
    "        'is_split': False,\n",
    "        'use_raw_split': False,\n",
    "        'split_dataset':None\n",
    "    },\n",
    "    {\n",
    "        'file': '/Volumes/davidh-ssd/manual_ips_mview.txt',\n",
    "        'is_split': False,\n",
    "        'use_raw_split': False,\n",
    "        'split_dataset':None\n",
    "    }\n",
    "]\n",
    "\n",
    "res = defaultdict(list)\n",
    "for task in tasks:\n",
    "    for k,v in parse_click_log(**task).items():\n",
    "        res[k].extend(v)\n",
    "        res[k] = sorted(res[k], key=lambda x: x[0][0])\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b: Parse click log (withing split images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_within_view_clicks(file, fold_split=8):\n",
    "    \n",
    "    with open(file, 'r') as fd:\n",
    "        ct = fd.read()\n",
    "\n",
    "    # remove comments and empty lines\n",
    "    ct = '\\n'.join(filter(lambda l: not l.strip().startswith('#') and not l.strip()=='', ct.split('\\n')))\n",
    "    \n",
    "    # split on !headers\n",
    "    blocks = list(filter(lambda c: c.strip() != '', ct.split('!')))\n",
    "    \n",
    "    res_pair = defaultdict(list)\n",
    "    \n",
    "    for block in blocks:\n",
    "\n",
    "        header, clicks = block.split('\\n', 1)\n",
    "        vid, vid = p_header.match(header.strip()).groups()\n",
    "\n",
    "        clicks = list(map(str.strip, filter(lambda c: c.strip() != '', clicks.split('---\\n'))))\n",
    "        \n",
    "        for click in clicks:\n",
    "\n",
    "            cli = click.split('\\n')\n",
    "            \n",
    "            locs = [p_click.match(c.strip()).groups() \n",
    "                      for c in cli if (\n",
    "                          int(p_click.match(c.strip()).groups()[0]) // fold_split\n",
    "                      ) == int(vid)]\n",
    "\n",
    "            locs = [(int(vid_), np.array([*map(float, pxa.strip('()').split(','))])) for vid_, _, pxa in locs]\n",
    "\n",
    "            for ((vida, loc1), (vidb, loc2)) in combinations(locs, 2):\n",
    "                if vida ^ vidb in [2**n for n in range(3)]:\n",
    "                    res_pair[(vida, vidb)].append((loc1, loc2))\n",
    "    return res_pair\n",
    "\n",
    "tasks = [\n",
    "    {\n",
    "        'file': '/Users/david/Desktop/ips_within_split_angle1.txt',\n",
    "    },\n",
    "    {\n",
    "        'file': '/Users/david/Desktop/ips_within_split_angle2.txt',\n",
    "    },\n",
    "]\n",
    "\n",
    "res = defaultdict(list)\n",
    "for task in tasks:\n",
    "    for k,v in parse_within_view_clicks(**task).items():\n",
    "        res[k].extend(v)\n",
    "        res[k] = sorted(res[k], key=lambda x: x[0][0])\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Map manual clicks to closest IP in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# parameters for outlier removal\n",
    "# roughly based on http://docs.pointclouds.org/1.7.1/classpcl_1_1_statistical_outlier_removal.html#details\n",
    "sd_mult_t = 3.0 # how many sds pixel-click pairs may devate from mean until they are removed\n",
    "p_estim = 0.8 # quantile of pixel-click pairs to use for mean, sd estimation\n",
    "\n",
    "# format string for MVR interest point files \n",
    "#ip_fstring = '/Volumes/davidh-ssd/BS_TEST/interestpoints/tpId_0_viewSetupId_{}.beads.ip.txt'\n",
    "ip_fstring = '/Volumes/davidh-ssd/BS_TEST/interestpoints/new_tpId_0_viewSetupId_{}.beads.ip.txt'\n",
    "\n",
    "res_ip = {}\n",
    "for vid_a, vid_b in res.keys():\n",
    "\n",
    "    # build kdtrees for real ips\n",
    "    df_a = pd.read_csv(ip_fstring.format(vid_a), sep='\\t')\n",
    "    locs_a = np.array(df_a[['x', 'y', 'z']])\n",
    "    kd_a = KDTree(locs_a)\n",
    "    df_b = pd.read_csv(ip_fstring.format(vid_b), sep='\\t')\n",
    "    locs_b = np.array(df_b[['x', 'y', 'z']])\n",
    "    kd_b = KDTree(locs_b)\n",
    "\n",
    "    res_pair = [] # pixel point pairs from IPs\n",
    "    manual_pair = [] # pixel point pairs from clicks\n",
    "    # find closest neighbours\n",
    "    for point_a, point_b in res[(vid_a, vid_b)]:\n",
    "        d_a, idx_a = kd_a.query(point_a)\n",
    "        d_b, idx_b = kd_b.query(point_b)\n",
    "        # keep pair only if we find reasonable match (distance of both < d_thresh)\n",
    "        # NB: removed for statistical outlier removal\n",
    "        #if (d_a < d_thresh and d_b < d_thresh):\n",
    "        res_pair += [(locs_a[idx_a], locs_b[idx_b])]\n",
    "        manual_pair += [(point_a, point_b)]\n",
    "\n",
    "    # filter outliers    \n",
    "    # vid_a: get mean and sd of pixel-clicked coordinates\n",
    "    ds = [l1 - l2 for (l1,_),(l2,_) in zip(res_pair, manual_pair)]\n",
    "    mu, sd = (np.mean(np.array(sorted(ds, key= lambda p: np.linalg.norm(p))[:int(len(ds)*p_estim)]), axis=0),\n",
    "              np.std(np.array(sorted(ds, key= lambda p: np.linalg.norm(p))[:int(len(ds)*p_estim)]), axis=0))\n",
    "    \n",
    "    # remember indices of good points\n",
    "    idxes_good = set([idx for idx, d in enumerate(ds) if np.all(np.abs(mu-d) < sd * sd_mult_t)])\n",
    "\n",
    "    # same for vid_b\n",
    "    ds = [l1 - l2 for (_, l1),(_, l2) in zip(res_pair, manual_pair)]\n",
    "    mu, sd = (np.mean(np.array(sorted(ds, key= lambda p: np.linalg.norm(p))[:int(len(ds)*p_estim)]), axis=0),\n",
    "              np.std(np.array(sorted(ds, key= lambda p: np.linalg.norm(p))[:int(len(ds)*p_estim)]), axis=0))\n",
    "    \n",
    "    # good points for both views\n",
    "    idxes_good &= set([idx for idx, d in enumerate(ds) if np.all(np.abs(mu-d) < sd * sd_mult_t)])\n",
    "    \n",
    "    # keep filtered IP coordinate pairs\n",
    "    res_pair_filt = [rp for idx,rp in enumerate(res_pair) if idx in idxes_good] \n",
    "    res_ip[(vid_a, vid_b)] = res_pair_filt\n",
    "\n",
    "# quick check: how many pairs remain?\n",
    "for k, v in res_ip.items():\n",
    "    print(k, len(v), \"!!!\" if len(v) < 4 else \"\")\n",
    "    \n",
    "#res_ip[(6,7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a: Creating Interest Point and Correspondence files (from scratch)\n",
    "\n",
    "Use this to create new IPs for BigStitcher (e.g. after manually selecting them in image-pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create BigSticher Interest point files\n",
    "# and xml-chunk to be added to dataset.xml\n",
    "\n",
    "# where to put ip files\n",
    "ip_out_fstring = '/Volumes/davidh-ssd/BS_TEST/interestpoints/tpId_0_viewSetupId_{}.manual.ip.txt'\n",
    "ip_out_corr_fstring = '/Volumes/davidh-ssd/BS_TEST/interestpoints/tpId_0_viewSetupId_{}.manual.corr.txt'\n",
    "\n",
    "# xml-interestpoint description\n",
    "rel_ip_out_fstring = 'interestpoints/tpId_0_viewSetupId_{}.manual'\n",
    "\n",
    "# collect ips for views\n",
    "res_singlevid = defaultdict(list)\n",
    "res_corr = defaultdict(list) # save correspondence ids\n",
    "ids = defaultdict(int) # running counts\n",
    "for (vid_a, vid_b), ips in res_ip.items():\n",
    "    for (ip_a, ip_b) in ips:\n",
    "        new_a = False\n",
    "        id_ma = add_array_set(res_singlevid[vid_a], ip_a, ids[vid_a])\n",
    "        if id_ma == ids[vid_a]: # new ip\n",
    "            ids[vid_a] += 1\n",
    "            new_a = True\n",
    "            \n",
    "        new_b = False\n",
    "        id_mb = add_array_set(res_singlevid[vid_b], ip_b, ids[vid_b])\n",
    "        if id_mb == ids[vid_b]:\n",
    "            ids[vid_b] += 1\n",
    "            new_b = True\n",
    "            \n",
    "        # remember correspondences\n",
    "        if new_a or new_b:\n",
    "            res_corr[vid_a].append((id_ma, (vid_b, id_mb)))\n",
    "            res_corr[vid_b].append((id_mb, (vid_a, id_ma)))\n",
    "            \n",
    "        # we already have both IPs, but are they correspondences yet?\n",
    "        else:\n",
    "            found = False\n",
    "            for (id_ma1, (vid_b1, id_mb1)) in res_corr[vid_a]:\n",
    "                if id_ma1 == id_ma and id_mb1 == id_mb:\n",
    "                    found = True\n",
    "            if not found:\n",
    "                res_corr[vid_a].append((id_ma, (vid_b, id_mb)))\n",
    "                res_corr[vid_b].append((id_mb, (vid_a, id_ma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b: Creating Interest Point and Correspondence files (merge into existing)\n",
    "\n",
    "Use this to merge 'within-image' correspondences we got after splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create BigSticher Interest point files\n",
    "# and xml-chunk to be added to dataset.xml\n",
    "\n",
    "# where to put ip files\n",
    "ip_out_fstring = '/Volumes/davidh-ssd/BS_TEST/interestpoints/new_tpId_0_viewSetupId_{}.manual.ip.txt'\n",
    "ip_out_corr_fstring = '/Volumes/davidh-ssd/BS_TEST/interestpoints/new_tpId_0_viewSetupId_{}.manual.corr.txt'\n",
    "\n",
    "# xml-interestpoint description\n",
    "rel_ip_out_fstring = 'interestpoints/tpId_0_viewSetupId_{}.manual'\n",
    "\n",
    "# collect ips for views\n",
    "res_singlevid = defaultdict(list)\n",
    "res_corr = defaultdict(list) # save correspondence ids\n",
    "ids = defaultdict(int) # running counts\n",
    "\n",
    "\n",
    "for (vid_a, vid_b), ips in res_ip.items():\n",
    "    \n",
    "    if not vid_a in res_singlevid:\n",
    "        if os.path.exists(ip_out_fstring.format(vid_a)):\n",
    "            res_singlevid[vid_a] = [v for v in pd.read_csv(ip_out_fstring.format(vid_a), sep='\\t')[['x', 'y', 'z']].values]\n",
    "            ids[vid_a] = len(res_singlevid[vid_a])\n",
    "        if os.path.exists(ip_out_corr_fstring.format(vid_a)):\n",
    "            res_corr[vid_a] = [(int(v[0]),(int(v[1]), int(v[2]))) for v in pd.read_csv(ip_out_corr_fstring.format(vid_a), sep='\\t')[[\n",
    "                'id', 'corresponding_viewsetup_id', 'corresponding_id']].values]\n",
    "        \n",
    "    if not vid_b in res_singlevid:\n",
    "        if os.path.exists(ip_out_fstring.format(vid_b)):\n",
    "            res_singlevid[vid_b] = [v for v in pd.read_csv(ip_out_fstring.format(vid_b), sep='\\t')[['x', 'y', 'z']].values]\n",
    "            ids[vid_b] = len(res_singlevid[vid_b])\n",
    "        if os.path.exists(ip_out_corr_fstring.format(vid_b)):\n",
    "            res_corr[vid_b] = [(int(v[0]),(int(v[1]), int(v[2]))) for v in pd.read_csv(ip_out_corr_fstring.format(vid_b), sep='\\t')[[\n",
    "                'id', 'corresponding_viewsetup_id', 'corresponding_id']].values]\n",
    "    \n",
    "    for (ip_a, ip_b) in ips:\n",
    "        new_a = False\n",
    "        id_ma = add_array_set(res_singlevid[vid_a], ip_a, ids[vid_a])\n",
    "        if id_ma == ids[vid_a]: # new ip\n",
    "            ids[vid_a] += 1\n",
    "            new_a = True\n",
    "        \n",
    "        new_b = False\n",
    "        id_mb = add_array_set(res_singlevid[vid_b], ip_b, ids[vid_b])\n",
    "        if id_mb == ids[vid_b]:\n",
    "            new_b = True\n",
    "            ids[vid_b] += 1\n",
    "            \n",
    "        if new_a or new_b:\n",
    "            # remember correspondences\n",
    "            res_corr[vid_a].append((id_ma, (vid_b, id_mb)))\n",
    "            res_corr[vid_b].append((id_mb, (vid_a, id_ma)))\n",
    "        \n",
    "        # we already have both IPs, but are they correspondences yet?\n",
    "        else:\n",
    "            found = False\n",
    "            for (id_ma1, (vid_b1, id_mb1)) in res_corr[vid_a]:\n",
    "                if id_ma1 == id_ma and id_mb1 == id_mb:\n",
    "                    found = True\n",
    "            if not found:\n",
    "                res_corr[vid_a].append((id_ma, (vid_b, id_mb)))\n",
    "                res_corr[vid_b].append((id_mb, (vid_a, id_ma)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Save IP files and print XML stub\n",
    "Save the IPs anc correspondences created above, print an XML stub to include in ```dataset.xml``` if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write ip file\n",
    "for vid in res_singlevid.keys():\n",
    "    df = pd.DataFrame(np.hstack(\n",
    "        (np.expand_dims(np.arange(len(res_singlevid[vid])),1),\n",
    "         np.array(res_singlevid[vid]))\n",
    "    ), columns=['id', 'x', 'y', 'z'])\n",
    "    df['id'] = df['id'].astype(np.int)\n",
    "    df.to_csv(ip_out_fstring.format(vid), index=False, sep='\\t')\n",
    "    \n",
    "    # dump corresondences\n",
    "    with open(ip_out_corr_fstring.format(vid), 'w') as fd:\n",
    "        fd.write('id\\tcorresponding_timepoint_id\\tcorresponding_viewsetup_id\\tcorresponding_label\\tcorresponding_id\\n')\n",
    "        for (id_a, (vid_b, id_b)) in res_corr[vid]:\n",
    "            fd.write(f'{id_a}\\t0\\t{vid_b}\\tmanual\\t{id_b}\\n')\n",
    "        \n",
    "\n",
    "# create XML for new ips\n",
    "xml = Element('ViewInterestPoints')\n",
    "for vid in res_singlevid.keys():\n",
    "    SubElement(xml, 'ViewInterestPointsFile',\n",
    "               timepoint='0', setup=str(vid), label='manual', params='manually picked'\n",
    "              ).text=rel_ip_out_fstring.format(vid)\n",
    "\n",
    "# for pretty print: write to in-memory file, prettyprint with minidom\n",
    "io = BytesIO()\n",
    "ElementTree(xml).write(io)\n",
    "io.seek(0)\n",
    "print(parse(io).toprettyxml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# manual inspection in 3d plots\n",
    "\n",
    "pairs = list(res_ip.values())[1]\n",
    "xs, ys, zs = [], [], []\n",
    "for l1, l2 in pairs:\n",
    "    d = l1 - l2\n",
    "    xs.append(d[0])\n",
    "    ys.append(d[1])\n",
    "    zs.append(d[2])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(xs, ys, zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: re-create correspondences after splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import product, count, combinations\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# old, non-split IPs\n",
    "ip_old_fstring = '/Volumes/davidh-ssd/BS_TEST/interestpoints/tpId_0_viewSetupId_{}.manual.ip.txt'\n",
    "ip_old_corr_fstring = '/Volumes/davidh-ssd/BS_TEST/interestpoints/tpId_0_viewSetupId_{}.manual.corr.txt'\n",
    "\n",
    "# split IPs\n",
    "ip_new_fstring = '/Volumes/davidh-ssd/BS_TEST/interestpoints/new_tpId_0_viewSetupId_{}.manual.ip.txt'\n",
    "xml_new = '/Volumes/davidh-ssd/BS_TEST/dataset_prealign.split.xml'\n",
    "\n",
    "# how much tiles a view was split into\n",
    "fold_split = 8\n",
    "\n",
    "# get all registrations from XML\n",
    "root = ElementTree().parse(xml_new)\n",
    "vrs = root.findall('./ViewRegistrations/ViewRegistration')\n",
    "angles = root.findall('./SequenceDescription/ViewSetups/ViewSetup/attributes/angle')\n",
    "\n",
    "def process_vid_pair(vid_a, vid_b):\n",
    "    res = defaultdict(list)\n",
    "    \n",
    "    # try to load non-split IPs, ignore if we do not have IPs for an image\n",
    "    try:\n",
    "        df_old_a = pd.read_csv(ip_old_fstring.format(vid_a), sep='\\t')\n",
    "        df_old_b = pd.read_csv(ip_old_fstring.format(vid_b), sep='\\t')\n",
    "        corr_old_a = pd.read_csv(ip_old_corr_fstring.format(vid_a), sep='\\t')\n",
    "        corr_old_b = pd.read_csv(ip_old_corr_fstring.format(vid_b), sep='\\t')\n",
    "    except FileNotFoundError as e:\n",
    "        return None\n",
    "        \n",
    "    # NB: dropping duplicates was necessary, as some correspondences were listed twice\n",
    "    # TODO: remove from non-split dataset\n",
    "    corr_old_a.drop_duplicates(inplace=True)\n",
    "    corr_old_b.drop_duplicates(inplace=True)\n",
    "    idxs = corr_old_a[corr_old_a['corresponding_viewsetup_id'] == vid_b]\n",
    "    \n",
    "    # just corresponding IPs in old views\n",
    "    # same ordering for both tables!\n",
    "    just_corr_a = df_old_a.iloc[list(idxs.id.values)]\n",
    "    just_corr_b = df_old_b.iloc[list(idxs.corresponding_id.values)]\n",
    "    just_corr_a = just_corr_a.reset_index(drop=True)\n",
    "    just_corr_b = just_corr_b.reset_index(drop=True)\n",
    "\n",
    "    # count number of good pairs (should be 4)\n",
    "    ctr_old = 0\n",
    "    \n",
    "    for (vid_new_1, vid_new_2) in product(np.arange(vid_a*fold_split, vid_a*fold_split+fold_split),\n",
    "                                         np.arange(vid_b*fold_split, vid_b*fold_split+fold_split)):\n",
    "\n",
    "        # get relative index of views\n",
    "        vid_n1_rel = (vid_new_1 - vid_a*fold_split)\n",
    "        vid_n2_rel = (vid_new_2 - vid_b*fold_split)\n",
    "\n",
    "        # ignore \"diagonal\" for multiview\n",
    "        # views are flipped in y\n",
    "        # we want to compare 1-2, 3-4, not 1-3\n",
    "        if angles[vid_new_1].text != angles[vid_new_2].text and not (vid_n1_rel ^ vid_n2_rel) == 1:\n",
    "            continue\n",
    "                        \n",
    "        # ignore diagonal\n",
    "        # xor of relative view ids is power of 2 -> differ only along one direction\n",
    "        # NB: only works for fold_split = 8\n",
    "        if not((vid_n1_rel ^ vid_n2_rel) in [2**n for n in range(3)]):\n",
    "            continue\n",
    "\n",
    "        # get split IPs\n",
    "        df_new_a = pd.read_csv(ip_new_fstring.format(vid_new_1), sep='\\t')\n",
    "        df_new_b = pd.read_csv(ip_new_fstring.format(vid_new_2), sep='\\t')\n",
    "\n",
    "        # get view transformations of split views\n",
    "        vt1 = vrs[vid_new_1].findall('./ViewTransform')[-1]\n",
    "        affine1 = np.zeros((4,4))\n",
    "        affine1[:3] += np.array(list(vt1)[1].text.split()).astype(np.float).reshape((3,4))\n",
    "        affine1[3,3] = 1.0\n",
    "\n",
    "        vt2 = vrs[vid_new_2].findall('./ViewTransform')[-1]\n",
    "        affine2 = np.zeros((4,4))\n",
    "        affine2[:3] += np.array(list(vt2)[1].text.split()).astype(np.float).reshape((3,4))\n",
    "        affine2[3,3] = 1.0\n",
    "\n",
    "        # count number of correspondences\n",
    "        ctr = 0\n",
    "        for i1, v1 in enumerate(list(df_new_a[['x', 'y', 'z']].values)):\n",
    "            v1t = (affine1.dot(np.array(list(v1) + [1]))[:3])\n",
    "            for i2, v2 in enumerate(list(df_new_b[['x', 'y', 'z']].values)):\n",
    "                v2t = (affine2.dot(np.array(list(v2) + [1]))[:3])\n",
    "\n",
    "                # index of transformed IPs\n",
    "                corra = just_corr_a.iloc[np.linalg.norm(just_corr_a[['x', 'y', 'z']].values - v1t, axis=1) < 0.001]\n",
    "                ioa = corra.index\n",
    "                corrb = just_corr_b.iloc[np.linalg.norm(just_corr_b[['x', 'y', 'z']].values - v2t, axis=1) < 0.001]\n",
    "                iob = corrb.index\n",
    "\n",
    "                \"\"\"\n",
    "                # caught by dropping duplicates above\n",
    "                if ioa.size > 1:\n",
    "                    print((vid_a, vid_b))\n",
    "                    print(v1t)\n",
    "                    print(just_corr_a.iloc[np.linalg.norm(just_corr_a[['x', 'y', 'z']].values - v1t, axis=1) < 0.001])\n",
    "                    \n",
    "                if iob.size > 1:\n",
    "                    print((vid_a, vid_b))\n",
    "                    print(v2t)\n",
    "                    print(just_corr_b.iloc[np.linalg.norm(just_corr_b[['x', 'y', 'z']].values - v2t, axis=1) < 0.001])\n",
    "                \"\"\"\n",
    "                \n",
    "                # transformed IPs have the same index in original data -> correspondence re-found\n",
    "                if ioa.size == 1 and iob.size == 1:\n",
    "                    if ioa == iob:\n",
    "                        ctr += 1\n",
    "                        \n",
    "                        # IP ids in split views\n",
    "                        ida = df_new_a.iloc[i1].id\n",
    "                        idb = df_new_b.iloc[i2].id\n",
    "                        \n",
    "                        res[(vid_new_1, vid_new_2)].append((ida, idb))\n",
    "\n",
    "        # split pairs with enough points for an affine model -> good\n",
    "        if ctr >= 4:\n",
    "            ctr_old += 1\n",
    "    \n",
    "    '''\n",
    "    if ctr_old > 0:\n",
    "        print((vid_a, vid_b), ctr_old)\n",
    "    '''\n",
    "    \n",
    "    return ctr_old, res\n",
    "\n",
    "# doit in parallel, because inefficient\n",
    "pool = Pool()\n",
    "res = pool.starmap(process_vid_pair, combinations(range(24), 2))\n",
    "\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many old pairs have more than one good link \n",
    "list(zip(combinations(range(24), 2), res))\n",
    "len([1 for i in res if i is not None and i[0] > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: create new correspondence files and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build new correspondences tables\n",
    "\n",
    "pd.DataFrame().append({'id':1, 'corresponding_timepoint_id':1, 'corresponding_viewsetup_id':1,\n",
    "                       'corresponding_label': 1, 'corresponding_id':1}, ignore_index=True)\n",
    "\n",
    "res_per_view = defaultdict(lambda: pd.DataFrame(columns=['id', 'corresponding_timepoint_id', 'corresponding_viewsetup_id',\n",
    "                       'corresponding_label', 'corresponding_id']))\n",
    "for i in res:\n",
    "    if i is None:\n",
    "        continue\n",
    "    if i[0] == 0:\n",
    "        continue\n",
    "    \n",
    "    res_i = i[1]\n",
    "    \n",
    "    for ((va, vb), ips) in res_i.items():\n",
    "        for (ipa, ipb) in ips:\n",
    "            res_per_view[va] =  res_per_view[va].append(\n",
    "                {'id': ipa,\n",
    "                 'corresponding_timepoint_id':0,\n",
    "                 'corresponding_viewsetup_id':vb,\n",
    "                 'corresponding_label': 'manual',\n",
    "                 'corresponding_id':ipb},\n",
    "                ignore_index=True)\n",
    "            res_per_view[vb] =  res_per_view[vb].append(\n",
    "                {'id': ipb,\n",
    "                 'corresponding_timepoint_id':0,\n",
    "                 'corresponding_viewsetup_id':va,\n",
    "                 'corresponding_label': 'manual',\n",
    "                 'corresponding_id':ipa},\n",
    "                ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new correspondence table\n",
    "\n",
    "ip_new_corr_fstring = '/Volumes/davidh-ssd/BS_TEST/interestpoints/new_tpId_0_viewSetupId_{}.manual.corr.txt'\n",
    "\n",
    "for k, v in res_per_view.items():\n",
    "\n",
    "    df = v.astype({'id':np.int, 'corresponding_timepoint_id':np.int, 'corresponding_viewsetup_id':np.int,\n",
    "                       'corresponding_label': str, 'corresponding_id':np.int})\n",
    "    \n",
    "    df.to_csv(ip_new_corr_fstring.format(k), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- DONE ---\n",
    "\n",
    "# OLD/TEST code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any pairs with <4 correspondences?\n",
    "for k,v in res_corr.items():\n",
    "    counts = defaultdict(int)\n",
    "    for (_, (cid, _)) in v:\n",
    "        counts[cid] += 1\n",
    "    for k2, c in counts.items():\n",
    "        if c < 4:\n",
    "            print(k, k2, c, '!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all correspondences between the blocked tiles of a view pair\n",
    "dict(zip(combinations(range(24), 2), res))[(10,17)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_new = '/Volumes/davidh-ssd/BS_TEST/dataset_prealign.split.xml'\n",
    "\n",
    "# how much tiles a view was split into\n",
    "fold_split = 8\n",
    "\n",
    "# get all registrations from XML\n",
    "root = ElementTree().parse(xml_new)\n",
    "vrs = root.findall('./SequenceDescription/ViewSetups/ViewSetup/attributes/angle')\n",
    "vrs[0].text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
